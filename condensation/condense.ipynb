{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Papers\n",
    "[1] Tao Luo#, Zhi-Qin John Xu #, Zheng Ma, Yaoyu Zhang*, Phase diagram for two-layer ReLU neural networks at infinite-width limit, arxiv 2007.07497 (2020), Journal of Machine Learning Research (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/phasediagram2020.pdf), and in [arxiv](https://arxiv.org/abs/2007.07497). \n",
    "\n",
    "[2] Hanxu Zhou, Qixuan Zhou, Tao Luo, Yaoyu Zhang*, Zhi-Qin John Xu*, Towards Understanding the Condensation of Neural Networks at Initial Training. arxiv 2105.11686 (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/initial2105.11686.pdf), and in [arxiv](https://arxiv.org/abs/2105.11686), see slides and [video talk in Chinese](https://www.bilibili.com/video/BV1tb4y1d7CZ/?spm_id_from%253D333.999.0.0), NeurIPS2022. \n",
    "\n",
    "For more details, see [xuzhiqin condense](https://ins.sjtu.edu.cn/people/xuzhiqin/pubcondense.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import argparse\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense \n",
    "![Condense](./pic/condense.png)\n",
    "The above picture is the illstration of the condensation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Default configuration parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch 1D dataset Training')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')\n",
    "parser.add_argument('--optimizer', default='adam',\n",
    "                    help='optimizer: sgd | adam')\n",
    "parser.add_argument('--epochs', default=2000, type=int,\n",
    "                    metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--test_size',   default=10000, type=int,\n",
    "                    help='the test size for model (default: 10000)')\n",
    "parser.add_argument('--save', default='trained_nets',\n",
    "                    help='path to save trained nets')\n",
    "parser.add_argument('--save_epoch', default=10,\n",
    "                    type=int, help='save every save_epochs')\n",
    "parser.add_argument('--rand_seed', default=0, type=int,\n",
    "                    help='seed for random num generator')\n",
    "parser.add_argument('--t', type=float, default=2,\n",
    "                    help='parameter initialization distribution variance power(We first assume that each layer is the same width.)')\n",
    "parser.add_argument('--boundary', nargs='+', type=str, default=['-1', '1'],\n",
    "                    help='the boundary of 1D data')\n",
    "parser.add_argument('--training_size',   default=4, type=int,\n",
    "                    help='the training size for model (default: 1000)')\n",
    "parser.add_argument('--act_func_name', default='ReLU',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--hidden_layers_width',\n",
    "                    nargs='+', type=int, default=[1000])\n",
    "parser.add_argument('--input_dim',   default=1, type=int,\n",
    "                    help='the input dimension for model (default: 1)')\n",
    "parser.add_argument('--output_dim',   default=1, type=int,\n",
    "                    help='the output dimension for model (default: 1)')\n",
    "parser.add_argument('--device',   default='cuda', type=str,\n",
    "                    help='device used to train (cpu or cuda)')\n",
    "parser.add_argument('--plot_epoch',   default=100, type=int,\n",
    "                    help='step size of plotting interval (default: 1000)')\n",
    "parser.add_argument('--ini_path', type=str,\n",
    "                    default='')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   The storage path of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdirs(fn):  # Create directorys\n",
    "    if not os.path.isdir(fn):\n",
    "        os.makedirs(fn)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def create_save_dir(path_ini): \n",
    "    subFolderName = re.sub(r'[^0-9]', '', str(datetime.datetime.now()))\n",
    "    path = os.path.join(path_ini, subFolderName)\n",
    "    mkdirs(path)\n",
    "    mkdirs(os.path.join(path, 'output'))\n",
    "    return path\n",
    "\n",
    "\n",
    "args.path = create_save_dir(args.ini_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of training and test sets.\n",
    "In this part, we use the target function $f(x)=0.2*ReLU(x-1/3) + 0.2*ReLU (-x-1/3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x):  # Function to fit\n",
    "    # y = torch.sin(x*math.pi)\n",
    "    y = 0.2*torch.relu(-1/3+x) + 0.2*torch.relu(-1/3-x)\n",
    "    return y\n",
    "\n",
    "for i in range(2):\n",
    "    if isinstance(args.boundary[i], str):\n",
    "        args.boundary[i] = eval(args.boundary[i])\n",
    "\n",
    "args.test_input = torch.reshape(torch.linspace(args.boundary[0] - 0.5, args.boundary[1] + 0.5, steps=args.test_size), [args.test_size, 1])\n",
    "\n",
    "\n",
    "args.training_input = torch.reshape(torch.linspace(args.boundary[0], args.boundary[1], steps=args.training_size), [args.training_size, 1])\n",
    "args.test_target = get_y(args.test_input)\n",
    "args.training_target = get_y(args.training_input)\n",
    "\n",
    "\n",
    "def plot_target(args):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.plot(args.training_input.detach().cpu().numpy(),\n",
    "             args.training_target.detach().cpu().numpy(), 'b*', label='True')\n",
    "    plt.plot(args.test_input.detach().cpu().numpy(),\n",
    "             args.test_target.detach().cpu().numpy(), 'r-', label='Test')\n",
    "\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_target(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xtanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xtanh,self).__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x * nn.Tanh()(x) \n",
    "    \n",
    "class x2tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(x2tanh,self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x * x * nn.Tanh()(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network model and parameter initialization.\n",
    "\n",
    "Given $\\theta\\in \\mathbb{R}^M$, the FNN function $f_{\\theta}(\\cdot)$ is defined recursively. First, we denote $f^{[0]}_{\\theta}(x)=x$ for all $x\\in\\mathbb{R}^d$. Then, for $l\\in[L-1]$, $f^{[l]}_{\\theta}$ is defined recursively as \n",
    "$f^{[l]}_{\\theta}(x)=\\sigma (W^{[l]} f^{[l-1]}_{\\theta}(x)+b^{[l]})$, where $\\sigma$ is a non-linear activation function.\n",
    "Finally, we denote\n",
    "\\begin{equation*}\n",
    "    f_{\\theta}(x)=f(x,\\theta)=f^{[L]}_{\\theta}(x)=W^{[L]} f^{[L-1]}_{\\theta}(x)+b^{[L]}.\n",
    "\\end{equation*}\n",
    "\n",
    "The parameter initialization is under the Gaussian distribution as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta|_{l} \\sim N(0, \\frac{1}{m_{l}^t}),\n",
    "\\end{equation*}\n",
    "where the $l$ th layer parameters of $\\theta$ is the ordered pair $\\theta|_{l}=\\Big(W^{[l]},b^{[l]}\\Big),\\quad l\\in[L]$, $m_{l}$ is the width of the $l$ th layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, t, hidden_layers_width=[100],  input_size=20, num_classes: int = 1000, act_layer: nn.Module = nn.ReLU()):\n",
    "        super(Linear, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_width = hidden_layers_width\n",
    "        self.t = t\n",
    "        layers: List[nn.Module] = []\n",
    "        self.layers_width = [self.input_size]+self.hidden_layers_width\n",
    "        for i in range(len(self.layers_width)-1):\n",
    "            layers += [nn.Linear(self.layers_width[i],\n",
    "                                    self.layers_width[i+1]), act_layer]\n",
    "        layers += [nn.Linear(self.layers_width[-1], num_classes, bias=False)]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "\n",
    "        for obj in self.modules():\n",
    "            if isinstance(obj, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.normal_(obj.weight.data, 0, 1 /\n",
    "                                self.hidden_layers_width[0]**(self.t))\n",
    "                if obj.bias is not None:\n",
    "                    nn.init.normal_(obj.bias.data, 0, 1 /\n",
    "                                    self.hidden_layers_width[0]**(self.t))\n",
    "\n",
    "\n",
    "def get_act_func(act_func):\n",
    "    if act_func == 'Tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif act_func == 'Sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act_func == 'xTanh':\n",
    "        return xtanh()\n",
    "    elif act_func == 'x2Tanh':\n",
    "        return x2tanh()\n",
    "    else:\n",
    "        raise NameError('No such act func!')\n",
    "\n",
    "\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.t, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step training function.\n",
    "\n",
    "The training data set is denoted as  $S=\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}^{d'}$. For simplicity, we assume an unknown function $y$ satisfying $y(x_i)=y_i$ for $i\\in[n]$. The empirical risk reads as\n",
    "\\begin{equation*}\n",
    "    R_S(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\ell(f(x_i,\\theta),y(x_i)),\n",
    "\\end{equation*}\n",
    "where the loss function $\\ell(\\cdot,\\cdot)$ is differentiable and the derivative of $\\ell$ with respect to its first argument is denoted by $\\nabla\\ell(y,y^*)$. \n",
    "\n",
    "For a one-step gradient descent, we have, \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta_{t+1}=\\theta_t-\\eta\\nabla R_S(\\theta).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, loss_fn,  args):\n",
    "\n",
    "    model.train()\n",
    "    device = args.device\n",
    "    data, target = args.training_input.to(\n",
    "        device), args.training_target.to(device).to(torch.float)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = loss_fn(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, args):\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    with torch.no_grad():\n",
    "        data, target = args.test_input.to(\n",
    "            device), args.test_target.to(device).to(torch.float)\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "    return loss.item(), outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(path, loss_train, x_log=False):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    y2 = np.asarray(loss_train)\n",
    "    plt.plot(y2, 'k-', label='Train')\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.yscale('log')\n",
    "    if x_log == False:\n",
    "        fntmp = os.path.join(path, 'loss.jpg')\n",
    "\n",
    "    else:\n",
    "        plt.xscale('log')\n",
    "        fntmp = os.path.join(path, 'loss_log.jpg')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fntmp,dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the output figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_output(path, args, output, epoch):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.plot(args.training_input.detach().cpu().numpy(),\n",
    "             args.training_target.detach().cpu().numpy(), 'b*', label='True')\n",
    "    plt.plot(args.test_input.detach().cpu().numpy(),\n",
    "             output.detach().cpu().numpy(), 'r-', label='Test')\n",
    "\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    fntmp = os.path.join(path, 'output', str(epoch)+'.jpg')\n",
    "\n",
    "    plt.savefig(fntmp, dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The expression of orientations and amplitudes of a Two-Layer Network\n",
    "\n",
    "$$\n",
    "f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})= \\sum_{k=1}^m a_k \\sigma\\left(\\boldsymbol{w}_k^{\\top} \\boldsymbol{x}\\right)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{w}_k = (w_k,b_k)^T$ and $\\boldsymbol{x} =(x,1)^T$.\n",
    "\n",
    "The orientation of a neuron $\\boldsymbol{w}_k$ is defined as $\\frac{w_k}{\\|\\boldsymbol{w}_k\\|_2}$. The amplitude of a neuron $\\boldsymbol{w}_k$ is $|a_k|\\cdot\\|\\boldsymbol{w_k}\\|_2$\n",
    "\n",
    "$a_k^0\\sim \\mathcal{N}(0,\\beta_1^2),\\quad \\boldsymbol{w_k^0}\\sim\\mathcal{N}(0,\\beta_2^2\\boldsymbol{I}_d)$\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\gamma = \\lim_{m\\rightarrow \\infty}-\\frac{\\log\\beta_1\\beta_2/\\alpha}{\\log m}, \\quad \\gamma'=\\lim_{m\\rightarrow \\infty} -\\frac{\\log\\beta_1/\\beta_2}{\\log m}\n",
    "\\end{equation*}\n",
    "\n",
    "When $\\sigma(x)=\\mathrm{ReLU}(x)$, we have the following phase diagram.\n",
    "\n",
    "![phase](./pic/phase.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the orientation and amplitude of each neuron in different layer.\n",
    "def get_ori_A(checkpoint):\n",
    "\n",
    "    wei1 = checkpoint['features.0.weight'].squeeze()\n",
    "    bias = checkpoint['features.0.bias'].squeeze()\n",
    "    wei2 = checkpoint['features.2.weight'].squeeze()\n",
    "    wei = wei1 / (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "\n",
    "    bia = bias / (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "    ori = torch.sign(bia) * torch.acos(wei)\n",
    "    A = wei2 * (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "\n",
    "    return ori, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the orientation and amplitude of each neuron at the beginning and end of training.\n",
    "def get_ori_A_list(checkpoint_list):\n",
    "\n",
    "    if not isinstance(checkpoint_list, list):\n",
    "        ori, A=get_ori_A(checkpoint_list)\n",
    "        return [ori], [A]\n",
    "\n",
    "    else:\n",
    "        ori_ini, A_ini = get_ori_A(checkpoint_list[0])\n",
    "\n",
    "        ori, A = get_ori_A(checkpoint_list[1])\n",
    "\n",
    "        return [ori_ini, ori], [A_ini, A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(path, ori, A, args, nota=''):\n",
    "    \"\"\"\n",
    "    It plots the feature of the input data\n",
    "    \n",
    "    :param path: the path to save the figure\n",
    "    :param ori: the feature orientation\n",
    "    :param A: the feature amplitude\n",
    "    :param args: the parameters of the model\n",
    "    \"\"\"\n",
    "\n",
    "    if args.input_dim == 1:\n",
    "\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "\n",
    "        if len(ori) == 1:\n",
    "\n",
    "            ori = ori[0].squeeze().detach().cpu().numpy()\n",
    "            A = A[0].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        elif len(ori) == 2:\n",
    "            ori_ini, ori = ori[0].squeeze().detach().cpu(\n",
    "            ).numpy(), ori[1].squeeze().detach().cpu().numpy()\n",
    "            A_ini, A = A[0].squeeze().detach().cpu().numpy(\n",
    "            ), A[1].squeeze().detach().cpu().numpy()\n",
    "\n",
    "            print(ori_ini.shape, A_ini.shape)\n",
    "\n",
    "            plt.scatter(ori_ini, abs(A_ini), color='cyan', label='ini feature')\n",
    "\n",
    "        else:\n",
    "            raise Exception('The length of the checkpoint list is less than or equal to two.')\n",
    "\n",
    "        # mkdirs(r'%s\\\\feature' % (path))\n",
    "        fn = mkdirs(os.path.join('%s'%path,'feature'))\n",
    "        plt.scatter(ori, abs(A), color='r', label='fin feature')\n",
    "        plt.xlim(-3.16,3.16)\n",
    "        plt.xlabel('orientation', fontsize=18)\n",
    "        plt.ylabel('amplitude', fontsize=18)\n",
    "        plt.legend(fontsize=18)\n",
    "        # fntmp = r'%s\\\\feature\\\\%s' % (path, nota)\n",
    "        # fntmp = os.path.join(fn,'%s'%nota)\n",
    "        # save_fig(plt, fntmp, pdf=False, x_log=False, y_log=True)\n",
    "        plt.savefig(os.path.join(fn,'%s'%nota))\n",
    "        plt.close()\n",
    "    else:\n",
    "        raise Exception('Input dimension must equal to 1!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![out05](./999.jpg)![out1](./3999.jpg)![out2](./1999.jpg) -->\n",
    "<center class=\"half\">\n",
    "    <img src=\"./pic/999.jpg\" width=\"300\"/><img src=\"./pic/3999.jpg\" width=\"300\"/><img src=\"./pic/1999.jpg\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"./pic/999.png\" width=\"300\"/><img src=\"./pic/3999.png\" width=\"300\"/><img src=\"./pic/1999.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "The above picture shows three typical case: linear(left), critical(middle), condense(right)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "args.t = 1\n",
    "args.lr = 0.05\n",
    "args.epochs = 10000\n",
    "args.save_epoch = 1000\n",
    "args.plot_epoch = 1000\n",
    "args.optimizer = 'sgd'\n",
    "args.act_func_name = 'ReLU'\n",
    "# args.path = create_save_dir(args.ini_path)\n",
    "args.savepath = create_save_dir(os.path.join(args.path, 't=%s'%args.t))\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.t, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n",
    "if args.optimizer=='sgd':\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "t0 = time.time()\n",
    "loss_training_lst=[]\n",
    "loss_test_lst = []\n",
    "for epoch in range(args.epochs+1):\n",
    "\n",
    "      model.train()\n",
    "      loss = train_one_step(\n",
    "        model, optimizer, loss_fn, args)\n",
    "      loss_test, output = test(\n",
    "          model, loss_fn, args)\n",
    "      loss_training_lst.append(loss)\n",
    "      loss_test_lst.append(loss_test)\n",
    "      if epoch % args.plot_epoch == 0:\n",
    "            print(\"[%d] loss: %.6f valloss: %.6f time: %.2f s\" %\n",
    "                  (epoch + 1, loss, loss_test, (time.time()-t0)))\n",
    "  \n",
    "      if (epoch+1) % (args.plot_epoch) == 0:\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=True)\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=False)\n",
    "\n",
    "          \n",
    "          para_now = copy.deepcopy(model.state_dict())\n",
    "          ori, A = get_ori_A_list([para_init,para_now])\n",
    "          plot_feature(args.savepath, ori, A, args,nota='%s'%epoch)\n",
    "\n",
    "          plot_model_output(args.savepath, args, output, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplicity of the activation function\n",
    "Suppose that $\\sigma(x)$ satisfies the following condition, there exists a $p\\in \\mathbb{N}$ and $p\\geq 1$, such that the $s$-th order derivatives $\\sigma^{(s)}(x)(0)=0$ for $s=1,2,\\cdots,p-1$ and $\\sigma^{(p)}(0) \\neq 0$, then we say $\\sigma(x)$ has the multiplicity $p$. \n",
    "\n",
    "* $tanh(x)$ has the multiplicity $1$.\n",
    "* $xtanh(x)$ has the multiplicity $2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "args.t = 2\n",
    "args.lr = 0.000001\n",
    "args.epochs = 500\n",
    "args.save_epoch = 100\n",
    "args.plot_epoch = 10\n",
    "args.optimizer = 'adam'\n",
    "args.act_func_name = 'Tanh'\n",
    "args.savepath = create_save_dir(os.path.join(args.path, 'initial%s'%args.act_func_name))\n",
    "\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.t, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n",
    "if args.optimizer=='sgd':\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "t0 = time.time()\n",
    "loss_training_lst=[]\n",
    "loss_test_lst = []\n",
    "for epoch in range(args.epochs+1):\n",
    "\n",
    "      model.train()\n",
    "      loss = train_one_step(\n",
    "        model, optimizer, loss_fn, args)\n",
    "      loss_test, output = test(\n",
    "          model, loss_fn, args)\n",
    "      loss_training_lst.append(loss)\n",
    "      loss_test_lst.append(loss_test)\n",
    "      if epoch % args.plot_epoch == 0:\n",
    "            print(\"[%d] loss: %.6f valloss: %.6f time: %.2f s\" %\n",
    "                  (epoch + 1, loss, loss_test, (time.time()-t0)))\n",
    "  \n",
    "      if (epoch+1) % (args.plot_epoch) == 0:\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=True)\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=False)\n",
    "\n",
    "          \n",
    "          para_now = copy.deepcopy(model.state_dict())\n",
    "          ori, A = get_ori_A_list([para_init,para_now])\n",
    "          plot_feature(args.savepath, ori, A, args,nota='%s'%epoch)\n",
    "\n",
    "          plot_model_output(args.savepath, args, output, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is about initial condense in different activation functions with different multiplicity.\n",
    "\n",
    "# Task: \n",
    "try to find the relation between the number of the directions in initial condense and the multiplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
